---
layout: base
use-site-title: true
---

</head>
{% include header.html type="page" %}
<div class="container" role="main">
  <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
	<img src="img/Screenshot%20(113).png" alt="Logo" style="width:400px;height:400px;">

	      
	<P CLASS="western" STYLE="margin-bottom: 0in">Objective of the
project: “The ImageNet for the blind”</P>
<P CLASS="western" STYLE="margin-bottom: 0in">The aim is building,
annotating and create a dataset that could be known as the “ImageNet”
for the blind, i.e., a dataset of images taken from a wearable camera
worn by blind people (or with their phones), that can later
facilitate image captioning and question answering to help them be
more independent in their daily lives.</P>
<P CLASS="western" STYLE="margin-bottom: 0in">
</P>
<P CLASS="western" STYLE="margin-bottom: 0in">We seek:</P>
<P CLASS="western" STYLE="margin-bottom: 0in">1. Blind volunteers
that are happy to share pictures of their lives with a wearable
mini-camera/phone in the situations they feel they would need most
help with, and they could ask a question about the picture that can
be answered by an artificial intelligent neural network model. The
data will be made public for research purposes only and to build the
training model so that the scientific community can improve upon such
models. Once enough data has been collected, the application to be
built can help describe with text the surrounding to the blind person
in the most needed times or in most beautiful times. We think people
who can benefit the most from artificial intelligence are people with
special needs, and that is the motivation to build datasets and AI
models that can make cities more visible simply by touching a button
of a wearable camera or their phone. Its activation will translate
the scene into a vocal description, helping to give the blind person
context to the environment around it or even answer questions posed
by the user.  More concretely, the idea is that we would train a
model to transform images to the relevant information that the blind
person would need from the images surrounding him and at the time of
saving, would also be nice to annotate if possible some context for
the blind person (what would he like to be explained to him, e.g. if
he is in a supermarket, in what food section I am? or in a new hotel,
in what direction should I walk towards the reception? The blind user
may provide context for each situation when he takes a picture where
he thinks he would benefit from a textual description of the
environment or a question answering application about his
surroundings. This information should be answerable by just looking
at the images of the wearable camera (or his phone, as he prefers).</P>
<P CLASS="western" STYLE="margin-bottom: 0in">Do you have blind
volunteers that would be happy to wear a Google Clips camera and save
images every time he/she finds a description of the environment as
image captioning, as well as the questions he would like to be
answered (or just use your phone)? We would provide them the camera
if he is happy to wear it, otherwise, your mobile phone will suffice.</P>
<P CLASS="western" STYLE="margin-bottom: 0in"><BR>
</P>
<P CLASS="western" STYLE="margin-bottom: 0in">2. AI Researcher: to
collect, annotate, document and publish the dataset and baseline
algorithms for image captioning from the collected dataset. The
dataset should be similar to MS-COCO (at least with the captions for
each image and possibly answers to questions written by the blind
person).</P>
<P CLASS="western" STYLE="margin-bottom: 0in"><BR>
</P>
<P CLASS="western" STYLE="margin-bottom: 0in">Interested? Send your
resume/motivation to natalia [dot] diaz [at] ensta [dot] fr</P>      
	      
	      
	      




	      
	      
	      
	      
	      
	</div>
  </div>
</div>

