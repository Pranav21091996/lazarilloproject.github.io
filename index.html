---
layout: base
use-site-title: true
---

</head>
{% include header.html type="page" %}
<div class="container" role="main">
  <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
	<img src="img/Screenshot%20(113).png" alt="Logo" style="width:200px;height:200px;">
	      <body lang="en" dir="ltr">	      
	      <p align="left" style="margin-bottom: 0in; line-height: 115%; page-break-before: auto; page-break-after: auto">
<b>Objective of Lazarillo Project</b></p>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
<p style="margin-bottom: 0in; line-height: 115%">The aim is building,
annotating and create a dataset, the “ImageNet” for the blind,
i.e., a dataset of images taken from a wearable camera (or phone if
preferred) by visually impaired people, that can later facilitate
image captioning and question answering to help them be more
independent in their daily lives. The aim of project Lazarillo is
developing this mobile application that has as end-user the visually
impaired.</p>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
<p style="margin-bottom: 0in; line-height: 115%">Once enough data has
been collected, the application will help describe, with text or
speech, the surroundings to the blind person, in the most needed
times, or in most beautiful moments. &nbsp;We think people who can
benefit the most from AI are people with special needs, and that is
the motivation to build datasets and models that can make cities more
visible simply by touching a button of a wearable camera or phone.
Its activation will translate the scene into a vocal description,
helping to give the blind person context about the environment around
him, or even answer questions posed by the user.</p>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
<p align="justify" style="margin-bottom: 0in; line-height: 0.2in"><b>Lazarillo
Application Use Cases</b></p>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
<p style="margin-bottom: 0in; line-height: 115%">1. The blind user
may choose to send for processing only the textual description of the
situation that the image its wearable camera takes, or also the
visual description, depending on the degree of complexity of help he
wants to obtain from the application, or the degree of privacy or
speed he requires for the answer.</p>
<p style="margin-bottom: 0in; line-height: 115%">2. Only elements
relevant to the situation in which the user specifically requires
help will be processed. In such situations, the user will share the
image of his environment through the wearable camera and then will
receive, through an audio, the caption (a <i>legend</i> or <i>subtitle)</i>
that would accompany the image of the real situation where he is
located; that is, a description of e.g., objects, people and the
general context that surrounds him in his current enviroment. The
application can also provide answers to questions about a given
image.</p>
<p align="left" style="margin-bottom: 0.17in; line-height: 0.23in"><font face="SimSun, serif"><font size="4" style="font-size: 13pt"><br/>
<span lang="en">&nbsp;</span></font></font></p>
<p align="justify" style="margin-bottom: 0in; line-height: 0.2in"><b>We
seek:</b></p>
<p style="margin-bottom: 0in; line-height: 115%"><b>AI Researchers
and engineers: </b>to collect, annotate, document and publish
datasets, and develop models for image captioning and visual question
answering (VQA) from the collected dataset. The dataset should be
similar to <font color="#1155cc"><u><a href="http://cocodataset.org/">MS-COCO</a></u></font>
(at least with captions per image and possibly answers to questions
-posed or not by the blind person). If you are willing to contribute,
join ContinualAI.org Slack and ask Natalia Diaz to add you to the
channel.</p>
<p style="margin-bottom: 0in; line-height: 0.15in"><br/>

</p>
<p style="margin-bottom: 0in; line-height: 115%"><b>Visually
impaired/ blind volunteers. </b>Do you know visually impaired people
that would be happy to wear the <font color="#1155cc"><u><a href="https://www.theverge.com/2018/2/27/17055618/google-clips-smart-camera-review">Google
Clips</a></u></font> camera and save images every time he would find
useful getting a description of the environment?<b> </b>We would
provide them the camera if they are comfortable wearing it
(otherwise, a mobile phone will suffice). Volunteers should be happy
to share pictures of their lives in the situations they feel they
would need most help with. In order to assist even more, they could
as well ask a question about the picture that could potentially be
answered by an AI model (i.e., as if we would provide <i>Siri</i>
with images). The data will be made public for research purposes and
so that the scientific community can build and improve upon such
models.</p>
<ol type="a">
	<li/>
<p style="margin-bottom: 0in; line-height: 115%">More
	concretely, we would train a model to transform images to the
	relevant information that the blind person would need from the
	images surrounding him. Therefore, at the time of saving, it would
	be great to annotate if possible some context for the blind person
	(what would he like to be told about the image? &nbsp;e.g. if he is
	in a supermarket, in what food section I am? or in a new hotel, In
	what direction should I walk to find the reception? The blind user
	may provide context for each situation when he takes a picture where
	he thinks he would benefit from a textual description of the
	environment. The idea is that this contextual information will help
	a question answering application improve responding questions about
	his surroundings in the future. This information should be
	answerable by just looking at the images of the wearable (or phone)
	camera. 
	</p>
</ol>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
<p align="left" style="margin-bottom: 0.17in; line-height: 0.23in"><font face="SimSun, serif"><font size="4" style="font-size: 13pt"><br/>
<span lang="en">&nbsp;</span></font></font></p>
<p align="left" style="margin-bottom: 0in; line-height: 115%"><br/>

</p>
	      </body>
	      
	</div>
  </div>
</div>

